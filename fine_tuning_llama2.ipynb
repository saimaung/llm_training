{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ATqg15ka1Gk"
      },
      "outputs": [],
      "source": [
        "!pip install datasets trl bitsandbytes peft\n",
        "!pip install transformers accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDaI8d6jbHkC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO6G4LNAZI0P"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_NAME = 'ChrisHayduk/Llama-2-SQL-Dataset'\n",
        "dataset = load_dataset(DATASET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEvlXR9VZiwC"
      },
      "outputs": [],
      "source": [
        "full_training_dataset = dataset['train']\n",
        "# randomize (not in order) by shuffling - to select first 1000 samples below to represent the whole dataset\n",
        "shuffled = full_training_dataset.shuffle()\n",
        "training_dataset = shuffled.select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALYlmWyicBtY"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # normalized floating 4 bit\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHtLpJ0xhaTV"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Meta one is gated via licensing - so use this one instead\n",
        "\n",
        "MODEL_NAME = 'NousResearch/Llama-2-7b-hf'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # switch between cpu and gpu automatically\n",
        ")\n",
        "\n",
        "model.config.use_cache = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1kkQ_gGp1ON"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_size = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8uZAK4AsgPm"
      },
      "outputs": [],
      "source": [
        "def concat_data_io(x):\n",
        "  concated = x['input'] + x['output']\n",
        "  #  text input must be of type `str` (single example),\n",
        "  # `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
        "  return tokenizer(concated, padding=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8GVbKXjuTwf"
      },
      "outputs": [],
      "source": [
        "concat_data_io(training_dataset[0])\n",
        "training_dataset = training_dataset.map(concat_data_io)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI55Np0R2APn"
      },
      "outputs": [],
      "source": [
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16, # rank, higher value means closer to fine tuning all parameters. lower value of R, faster fine tuning, not greatest results cos not fine tuning as many parameters\n",
        "    lora_alpha=32, # scaling factore used in matrix multiplication\n",
        "    # which layers we want to apply LoRA to.\n",
        "    # anything not in target_modules will be frozen\n",
        "    target_modules=[\n",
        "        'q_proj', # query projection\n",
        "        'k_proj', # key projection\n",
        "        'down_proj', # part of feed forward layer nn\n",
        "        'v_proj', # value projection\n",
        "        'gate_proj', # part of feed forward layer nn\n",
        "        'o_proj',\n",
        "        'up_proj' # part of feed forward layer nn\n",
        "    ],\n",
        "    lora_dropout=0.05, # to prevent overfitting - every iterations of training, turn off some of the nodes (set nodes to be 0)\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L8z0U-LE_qN"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config) # all layers except attention layers are frozen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLsRllhz9NsP"
      },
      "outputs": [],
      "source": [
        "generation_configuration = model.generation_config\n",
        "generation_configuration.pad_token_id = tokenizer.eos_token_id\n",
        "generation_configuration.eos_token_id = tokenizer.eos_token_id\n",
        "generation_configuration.max_new_tokens = 256\n",
        "generation_configuration.temperature = 0.7\n",
        "generation_configuration.top_p = 0.9\n",
        "generation_configuration.do_sample = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmJNxXzK9xPR"
      },
      "outputs": [],
      "source": [
        "def complete(prompt):\n",
        "  generation_configuration.max_new_tokens = 20\n",
        "  encoded = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
        "  with torch.inference_mode():\n",
        "    out = model.generate(\n",
        "        input_ids=encoded,\n",
        "        generation_config=generation_configuration,\n",
        "        repetition_penalty=2.0)\n",
        "  string_decoded = tokenizer.decode(out[0], clean_up_tokenization_spaces=True)\n",
        "  print(string_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjw8Si-l_bCh"
      },
      "outputs": [],
      "source": [
        "complete('Hello World, ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Inn4jWTAQ1p"
      },
      "outputs": [],
      "source": [
        "train_arguments = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4, # simulate a larger batch size\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    output_dir=\"fine_tuning\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoyWr-BZ_NGc"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=training_dataset,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    args=train_arguments\n",
        ")\n",
        "\n",
        "model.config.use_cache = False # for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VArOfvHWA-ch"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAOHf7_LBw8A"
      },
      "outputs": [],
      "source": [
        "evaluation_dataset = dataset['eval'].shuffle()\n",
        "\n",
        "sample_sql_question = evaluation_dataset[0]['input']\n",
        "correct_answer = evaluation_dataset[0]['output']\n",
        "\n",
        "complete(sample_sql_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehaXjqh1IoQc"
      },
      "outputs": [],
      "source": [
        "correct_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVcY7TpBGwTX"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "FINE_TUNED_MODEL_NAME='Llama-2-7b-multiple-experts-hf'\n",
        "model.save_pretrained(FINE_TUNED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKoGL6dcG4Yz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Replace 'your_model_name' with the desired name for your model\n",
        "model = AutoModelForCausalLM.from_pretrained(FINE_TUNED_MODEL_NAME, push_to_hub=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_NAME)\n",
        "\n",
        "# Set the repo owner and private key (if you want to make it public, set `private_key=None`)\n",
        "model.push_to_hub(FINE_TUNED_MODEL_NAME, repo=f'saiwaimaung/{FINE_TUNED_MODEL_NAME}', private_key=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOSveabSH30N"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(f'saiwaimaung/{FINE_TUNED_MODEL_NAME}')\n",
        "tokenizer = AutoTokenizer.from_pretrained(f'saiwaimaung/{FINE_TUNED_MODEL_NAME}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
